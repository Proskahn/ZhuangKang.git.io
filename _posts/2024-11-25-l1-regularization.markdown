---

layout: post
title: Does L1-Regularization Really Promote Sparsity?
date: 24.11.2024
description: Does $\ell_1$ regularization really reduce the number of non-zero parameters? My answer is, **I do not think so.**
img: Figure5.png
---

When I looked through web pages about L1-regularization, I found that many people claim L1 induces sparsity in neural networks. However, I also came across some posts on Stack Overflow asking, “Why doesn’t L1-regularization force the weight parameters to become zero?”

As part of my coursework, I investigated this topic through research and experimentation. If you were to ask me, “Does L1-regularization really reduce the number of non-zero parameters?”

My answer is: **I do not think so.**


Click the link below to read the full article.

https://medium.com/@k13659082460/does-l1-regularization-really-promote-sparsity-dc300a54c413